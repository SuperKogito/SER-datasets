
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Datasets &#8212; 🧠 SuperKogito/SER-datasets  documentation</title>
    <script>
      document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
      document.documentElement.dataset.theme = localStorage.getItem("theme") || "light"
    </script>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/tree_graph.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/social_media_sharing.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.23/css/jquery.dataTables.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="https://cdn.datatables.net/1.10.23/js/jquery.dataTables.min.js"></script>
    <script src="_static/js/main.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GPWJJB9TRK"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-GPWJJB9TRK');
    </script>

    <!-- Google Ads -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8554106698532684" crossorigin="anonymous"></script>
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60" data-default-mode="">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="#">
<p class="title">🧠 SuperKogito/SER-datasets</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    
    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://superkogito.github.io/index.html">Home<i class="fas fa-external-link-alt"></i></a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://superkogito.github.io/projects.html">Projects<i class="fas fa-external-link-alt"></i></a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://superkogito.github.io/blog.html">Blog<i class="fas fa-external-link-alt"></i></a>
    </li>
    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://superkogito.github.io/about.html">About Me<i class="fas fa-external-link-alt"></i></a>
    </li>
    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this site..." aria-label="Search this site..." autocomplete="off" >
</form>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/superkogito/ser-datasets" rel="noopener" target="_blank" title="GitHub"><span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          
            <div class="d-none d-md-block col-md-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage mt-5 pt-1 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Datasets
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contributing">
   Contributing
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#disclaimer">
   Disclaimer
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recommended-tools">
   Recommended tools
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          <main class="col-12 col-md-9  py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="datasets">
<h1>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">#</a></h1>
<p>Spoken Emotion Recognition Datasets: A collection of datasets for the purpose of emotion recognition/detection in speech.
The table is chronologically ordered and includes a description of the content of each dataset along with the emotions included.</p>
<table class="datatable table" id="id29">
<caption><span class="caption-text">SER-Datasets</span><a class="headerlink" href="#id29" title="Permalink to this table">#</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>Year</p></th>
<th class="head"><p>Content</p></th>
<th class="head"><p>Emotions</p></th>
<th class="head"><p>Format</p></th>
<th class="head"><p>Size</p></th>
<th class="head"><p>Language</p></th>
<th class="head"><p>Paper</p></th>
<th class="head"><p>Access</p></th>
<th class="head"><p>License</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/amu-cai/nEMO">nEmo</a></p></td>
<td><p>2024</p></td>
<td><p>3 hours of samples recorded with the participation of nine actors.</p></td>
<td><p>6 emotions: anger, fear, happiness, sadness, surprised, and neutral.</p></td>
<td><p>Audio</p></td>
<td><p>0.434 GB</p></td>
<td><p>Polish</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2404.06292">nEMO: Dataset of Emotional Speech in Polish</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://zenodo.org/records/10694370">EMOVOME</a></p></td>
<td><p>2024</p></td>
<td><p>999 spontaneous voice messages from 100 Spanish speakers, collected from real conversations on a messaging app.</p></td>
<td><p>Valence &amp; arrousal dimensions and 7 emotions: happiness, disgust, anger, surprise, fear, sadness, and neutral.</p></td>
<td><p>Audio</p></td>
<td><p>–</p></td>
<td><p>Spanish</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2403.02167">EMOVOME Database: Advancing Emotion Recognition in Speech Beyond Staged Scenarios</a></p></td>
<td><p>Partially open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://www.openslr.org/136/">EMNS</a></p></td>
<td><p>2023</p></td>
<td><p>1206 high quality labeled utterances by one female speaker (2-3 hours).</p></td>
<td><p>Anger, excitement, disgust, happiness, surprise, sadness, and neutral (plus sarcasm)</p></td>
<td><p>Audio</p></td>
<td><p>0.042 GB</p></td>
<td><p>English (British)</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2305.13137">EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://apache.org/licenses/LICENSE-2.0">Apache 2.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://rds.westernsydney.edu.au/Institutes/MARCS/2024/Christopher_Davis/">CAVES</a></p></td>
<td><p>2023</p></td>
<td><p>Full hd visual recordings of 10 native cantonese speakers uttering 50 sentences.</p></td>
<td><p>Anger, happiness, sadness, surprise, fear, disgust and neutral</p></td>
<td><p>Audio</p></td>
<td><p>47 GB</p></td>
<td><p>Chinese (cantonese)</p></td>
<td><p><a class="reference external" href="https://link.springer.com/article/10.3758/s13428-023-02270-7">A Cantonese Audio-Visual Emotional Speech (CAVES) dataset</a></p></td>
<td><p>Open</p></td>
<td><p>Available for research purposes only</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://data.mendeley.com/datasets/rdwn4bs5ky/2">BANSpEmo</a></p></td>
<td><p>2023</p></td>
<td><p>792 utterance recordings from 22 unprofessional speakers (11 males and 11 females) of six basic emotional reactions of two sets of sentences.</p></td>
<td><p>angry, disgusted, happy, surprised, sad, fear</p></td>
<td><p>Audio</p></td>
<td><p>0.555 GB</p></td>
<td><p>Bangla</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2312.14020">BANSpEmo: A Bangla Emotional Speech Recognition Dataset</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://data.mendeley.com/datasets/vsn37ps3rx/4">KBES</a></p></td>
<td><p>2023</p></td>
<td><p>900 audio signals from 35 actors (20 females and 15 males). Each emotion is represented with two intensity levels (low &amp; high)</p></td>
<td><p>angry, disgusted, happy, neutral, sad</p></td>
<td><p>Audio</p></td>
<td><p>0.337 GB</p></td>
<td><p>Bangla</p></td>
<td><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S2352340923008107">KBES: A dataset for realistic Bangla speech emotion recognition with intensity level</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/records/7091465">Hi, KIA</a></p></td>
<td><p>2022</p></td>
<td><p>A shared short Wakeup Word database focusing on perceived emotion in speech The dataset contains 488 Wakeup Word speech</p></td>
<td><p>angry, happy, sad, neutral</p></td>
<td><p>Audio</p></td>
<td><p>0.75 GB</p></td>
<td><p>Korean</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2211.03371">Hi, KIA: A Speech Emotion Recognition Dataset for Wake-Up Words</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://zenodo.org/records/6569824">Emozionalmente</a></p></td>
<td><p>2022</p></td>
<td><p>6902 labeled samples acted out by 431 amateur actors while verbalizing 18 different sentences</p></td>
<td><p>anger, disgust, fear, joy, sadness, surprise, neutral</p></td>
<td><p>Audio</p></td>
<td><p>0.581 GB</p></td>
<td><p>Italian</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://data.mendeley.com/datasets/t9h6p943xy/5">BanglaSER</a></p></td>
<td><p>2022</p></td>
<td><p>1467 Bangla speech-audio recordings by 34 non-professional participating actors (17 male and 17 female) from diverse age groups between 19 and 47 years.</p></td>
<td><p>angry, happy, neutral, sad, surprise</p></td>
<td><p>Audio</p></td>
<td><p>0.425 GB</p></td>
<td><p>Bangla</p></td>
<td><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S235234092200302X">BanglaSER: A speech emotion recognition dataset for the Bangla language</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://data.mendeley.com/datasets/t9h6p943xy/3">B-SER</a></p></td>
<td><p>2022</p></td>
<td><p>1224 speech-audio recordings by 34 non-professional participating actors (17 male and 17 female) from diverse age groups between 19 and 47 years.</p></td>
<td><p>angry, happy, sad and surprise</p></td>
<td><p>Audio</p></td>
<td><p>0.363 GB</p></td>
<td><p>Bangla</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/records/6345107">Kannada</a></p></td>
<td><p>2022</p></td>
<td><p>468 audio samples, six different sentences, pronounced by thirteen people (four male and nine female), in five basic emotions plus one neutral emotion</p></td>
<td><p>Anger, Sadness, Surprise, Happiness, Fear, Neutral</p></td>
<td><p>Audio</p></td>
<td><p>0.1661 GB</p></td>
<td><p>Kannada</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://figshare.com/articles/media/Quechua_Collao_for_Speech_Emotion_Recognition/20292516">Quechua-SER</a></p></td>
<td><p>2022</p></td>
<td><p>12420 audio recordings (~15 hours) and their transcriptions by 7 native speakers.</p></td>
<td><p>Emotional labels using dimensions: valence, arousal, and dominance.</p></td>
<td><p>Audio</p></td>
<td><p>3.53 GB</p></td>
<td><p>Quechua Collao</p></td>
<td><p><a class="reference external" href="https://www.nature.com/articles/s41597-022-01855-9">A speech corpus of Quechua Collao for automatic dimensional emotion recognition</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://data.mendeley.com/datasets/cy34mh68j9/5">MESD</a></p></td>
<td><p>2022</p></td>
<td><p>864 audio files of single-word emotional utterances with Mexican cultural shaping.</p></td>
<td><p>6 emotions provides single-word utterances for anger, disgust, fear, happiness, neutral, and sadness.</p></td>
<td><p>Audio</p></td>
<td><p>0.097 GB</p></td>
<td><p>Spanish (Mexican)</p></td>
<td><p><a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/34891601/">The Mexican Emotional Speech Database (MESD): elaboration and assessment based on machine learning</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://zenodo.org/record/6573016#.ZAjy_9LMJpj">SyntAct</a></p></td>
<td><p>2022</p></td>
<td><p>Synthesized database with 997 utterances of three basic emotions and neutral expression based on rule-based manipulation for a diphone synthesizer which we release to the public</p></td>
<td><p>6 emotions: angry, bored, happy, neutral, sad and scared</p></td>
<td><p>Audio</p></td>
<td><p>0.941 GB</p></td>
<td><p>German</p></td>
<td><p><a class="reference external" href="http://felix.syntheticspeech.de/publications/synthetic_database.pdf">SyntAct: A Synthesized Database of Basic Emotions</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0">CC BY-SA 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://drive.google.com/drive/folders/1EKuWH8q178QOtFUYaNohdkZbBHQYAmhL">BEAT</a></p></td>
<td><p>2022</p></td>
<td><p>76-Hour and 30-Speaker of 4 different languages: English (60h), Chinese (12h), Spanish (2h) and Japanese (2h).</p></td>
<td><p>8 emotions: happiness, anger, disgust, sadness, contempt, surprise, fear, and neutral</p></td>
<td><p>Audio, Video</p></td>
<td><p>42 GB</p></td>
<td><p>English, Chinese, Spanish, Japanese</p></td>
<td><p><a class="reference external" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670605.pdf">A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis</a></p></td>
<td><p>Open</p></td>
<td><p>Non-commercial license</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/salute-developers/golos/tree/master/dusha">Dusha</a></p></td>
<td><p>2022</p></td>
<td><p>300000 audio recordings (~350 hours) of Russian speech, their transcripts and emotiomal labels. The dataset has two subsets: acted and real-life</p></td>
<td><p>4 emotions: angry, happy, sad and neutral. Arousal and valence metrics are also available.</p></td>
<td><p>Audio</p></td>
<td><p>58 GB</p></td>
<td><p>Russian</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2212.12266">Large Raw Emotional Dataset with Aggregation Mechanism</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://github.com/salute-developers/golos/blob/master/license/en_us.pdf">Public license with attribution and conditions reserved</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://mafw-database.github.io/MAFW/">MAFW</a></p></td>
<td><p>2022</p></td>
<td><p>10045 video-audio clips in the wild.</p></td>
<td><p>11 single-label emotion categories (anger, disgust, fear, happiness, neutral, sadness, surprise, contempt, anxiety, helplessness, and disappointment) and 32 multi-label emotion categories.</p></td>
<td><p>Audio, Video</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2208.00847">MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild</a></p></td>
<td><p>Restricted</p></td>
<td><p>Non-commercial research purposes</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://viem-ccy.github.io/EMOVIE/dataset_release.html">EMOVIE</a></p></td>
<td><p>2021</p></td>
<td><p>9724 samples with audio files and its emotion human-labeled annotation.</p></td>
<td><p>Polarity metrics (positive:+1, negative:-1)</p></td>
<td><p>Audio</p></td>
<td><p>0.572 GB</p></td>
<td><p>Chinese (Mandarin)</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2106.09317">EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/2.0/legalcode">CC BY-NC-SA 2.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/records/5427549">emoUERJ</a></p></td>
<td><p>2021</p></td>
<td><p>Ten sentences from eight actors, equally divided between genders, and they were free to choose the phrases for record audios in four emotions (377 audios).</p></td>
<td><p>happiness, anger, sadness or neutral</p></td>
<td><p>Audio</p></td>
<td><p>0.1051 GB</p></td>
<td><p>Portuguese (Brazilian)</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://zenodo.org/records/5525023">Thorsten-Voice Dataset 2021.06 emotional</a></p></td>
<td><p>2021</p></td>
<td><p>2400 normalized mono recordings by one person (Thorsten Müller) representing 300 sentences.</p></td>
<td><p>Amusement, Disgust Anger, Suprise and Neutral (plus drunk, whispering and sleepy states)</p></td>
<td><p>Audio</p></td>
<td><p>0.399 GB</p></td>
<td><p>German</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/publicdomain/zero/1.0/">CC0: Public Domain</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/Ethio2021/ASED_V1">ASED</a></p></td>
<td><p>2021</p></td>
<td><p>2474 recordings by 65 participants (25 females and 40 males)). Recordings were judged and rejected according to the opionion of eight judges.</p></td>
<td><p>Five emotions: anger, happiness, fear, sadness and neutral</p></td>
<td><p>Audio</p></td>
<td><p>0.135 GB</p></td>
<td><p>Amharic</p></td>
<td><p><a class="reference external" href="https://dl.acm.org/doi/10.1145/3529759">A New Amharic Speech Emotion Dataset and Classification Benchmark</a></p></td>
<td><p>Open</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://zenodo.org/records/5793223">ESCorpus-PE</a></p></td>
<td><p>2021</p></td>
<td><p>Spanish peruvian speech gathered from Spanish interviews, TV reports, political debate and testimonials. It contains 3749 utterances, 80 speakers (44 male and 36 female), created from Youtube audios</p></td>
<td><p>Valence, Arousal and Dominance</p></td>
<td><p>Audio</p></td>
<td><p>1.9 GB</p></td>
<td><p>Spanish (Peruvian)</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/record/6339787">SUBSECO</a></p></td>
<td><p>2021</p></td>
<td><p>7000 sentence-level utterances of the Bangla language, 20 professional actors (10 males and 10 females), recordings, 10 sentences for 7 target emotions.</p></td>
<td><p>Anger, Disgust, Fear, Happiness, Neutral, Sadness and Surprise</p></td>
<td><p>Audio</p></td>
<td><p>1.7 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://doi.org/10.1371/journal.pone.0250173">SUST Bangla Emotional Speech Corpus (SUBESCO): An audio-only emotional speech corpus for Bangla</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.kaggle.com/imsparsh/audio-speech-sentiment-analysis">Audio-Speech-Sentiment</a></p></td>
<td><p>2021</p></td>
<td><p>Audio Speech Sentiment Dataset</p></td>
<td><p>4 emotions provides audio recordings of spoken sentences for anger, happiness, sadness, and neutral emotions.</p></td>
<td><p>Audio</p></td>
<td><p>1.1 GB</p></td>
<td><p>English</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/publicdomain/zero/1.0/">CC0: Public Domain</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/tobefans/LSSED">LSSED</a></p></td>
<td><p>2021</p></td>
<td><p>LSSED: A Large-Scale Dataset and Benchmark for Speech Emotion Recognition</p></td>
<td><p>Anger, happiness, sadness, disappointment, boredom, disgust, excitement, fear, surprise, normal, and other.</p></td>
<td><p>Audio</p></td>
<td><p>90 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2102.01754">LSSED: A Large-Scale Spanish Emotional Speech Database for Speech Processing and Machine Learning</a></p></td>
<td><p>Restricted</p></td>
<td><p><a class="reference external" href="https://github.com/tobefans/LSSED/blob/main/EULA.pdf">-</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.kaggle.com/datasets/jesusrequena/mlend-spoken-numerals">MLEnd</a></p></td>
<td><p>2021</p></td>
<td><p>~32700 audio recordings files produced by 154 speakers. Each audio recording corresponds to one English numeral (from “zero” to “billion”)</p></td>
<td><p>Intonations: neutral, bored, excited and question</p></td>
<td><p>Audio</p></td>
<td><p>2.27 GB</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p>Unknown</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.kaggle.com/datasets/dejolilandry/asvpesdspeech-nonspeech-emotional-utterances">ASVP-ESD</a></p></td>
<td><p>2021</p></td>
<td><p>~13285 audio files collected from movies, tv shows and youtube containing speech and non-speech.</p></td>
<td><p>12 different natural emotions (boredom, neutral, happiness, sadness, anger, fear, surprise, disgust, excitement, pleasure, pain, disappointment) with 2 levels of intensity.</p></td>
<td><p>Audio</p></td>
<td><p>2 GB</p></td>
<td><p>Chinese, English, French, Russian and others</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p>Unknown</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://hltsingapore.github.io/ESD/">ESD</a></p></td>
<td><p>2021</p></td>
<td><p>29 hours, 3500 sentences, by 10 native English speakers and 10 native Chinese speakers.</p></td>
<td><p>5 emotions: angry, happy, neutral, sad, and surprise.</p></td>
<td><p>Audio,  Text</p></td>
<td><p>2.4 GB</p></td>
<td><p>Chinese, English</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/2010.14794.pdf">Seen And Unseen Emotional Style Transfer For Voice Conversion With A New Emotional Speech Dataset</a></p></td>
<td><p>Open</p></td>
<td><p>Academic License</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/record/4134758">MuSe-CAR</a></p></td>
<td><p>2021</p></td>
<td><p>40 hours, 6,000+ recordings of 25,000+ sentences by 70+ English speakers (see db link for details).</p></td>
<td><p>continuous emotion dimensions characterized using valence, arousal, and trustworthiness.</p></td>
<td><p>Audio, Video, Text</p></td>
<td><p>15 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/2101.06053.pdf">The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements</a></p></td>
<td><p>Restricted</p></td>
<td><p>Academic License &amp; Commercial License</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/vistec-AI/dataset-releases/releases/tag/v1">THAI SER</a></p></td>
<td><p>2021</p></td>
<td><p>The recordings are 41 hours, 36 minutes long (27,854 utterances), and were performed by 200 professional actors (112 female, 88 male).</p></td>
<td><p>5 main emotions assigned to actors: Neutral, Anger, Happiness, Sadness, and Frustration.</p></td>
<td><p>Audio</p></td>
<td><p>12 GB</p></td>
<td><p>Thai</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/records/4405783#.Yqjq_9JBxph">French Emotional Speech Database - Oréau</a></p></td>
<td><p>2020</p></td>
<td><p>79 utterances with 10 to 13 utterances pro emotion by 32 non-professional speakers.</p></td>
<td><p>7 emotions: sadness, anger, disgust, fear, surprise, joy, neutral.</p></td>
<td><p>Audio</p></td>
<td><p>0.264 GB</p></td>
<td><p>French</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://www.openslr.org/88/">Att-HACK</a></p></td>
<td><p>2020</p></td>
<td><p>25 speakers interpreting 100 utterances in 4 social attitudes, with 3-5 repetitions each per attitude for a total of around 30 hours of speech.</p></td>
<td><p>expressive speech in French, 100 phrases with multiple versions (3 to 5) in four social attitudes (friendly, distant, dominant and seductive).</p></td>
<td><p>Audio</p></td>
<td><p>6.6 GB</p></td>
<td><p>French</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2004.04410">Att-HACK: An Expressive Speech Database with Social Attitudes</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Podcast.html">MSP-Podcast corpus</a></p></td>
<td><p>2020</p></td>
<td><p>100 hours by over 100 speakers (see db link for details).</p></td>
<td><p>This corpus is annotated with emotional labels using attribute-based descriptors (activation, dominance and valence) and categorical labels (anger, happiness, sadness, disgust, surprised, fear, contempt, neutral and other).</p></td>
<td><p>Audio</p></td>
<td><p>13.4 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="http://www.interspeech2020.org/index.php?m=content&amp;c=index&amp;a=show&amp;catid=290&amp;id=684">The MSP-Conversation Corpus</a></p></td>
<td><p>Restricted</p></td>
<td><p>Academic License &amp; Commercial License</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.openslr.org/93/">AISHELL-3</a></p></td>
<td><p>2020</p></td>
<td><p>Roughly 85 hours of emotion-neutral recordings spoken by 218 native Chinese mandarin speakers and total 88035 utterances.</p></td>
<td><p>Neutral</p></td>
<td><p>Audio</p></td>
<td><p>19 GB</p></td>
<td><p>Chinese (Mandarin)</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2010.11567">AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://apache.org/licenses/LICENSE-2.0">Apache 2.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://doi.org/10.6084/m9.figshare.12498033">BEASC</a></p></td>
<td><p>2020</p></td>
<td><p>Bangla Emotional Audio-Speech Corpus</p></td>
<td><p>6 emotions provides Bangla spoken utterances for anger, happiness, sadness, fear, surprise, and neutral.</p></td>
<td><p>Audio</p></td>
<td><p>9 GB</p></td>
<td><p>Bangla</p></td>
<td><p><a class="reference external" href="https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:236649">BEASC: Bangla Emotional Audio-Speech Corpus - A Speech Emotion Recognition Corpus for the Low-Resource Bangla Language</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/emotiontts/emotiontts_open_db">emotiontts open db</a></p></td>
<td><p>2020</p></td>
<td><p>Recordings and their associated transcriptions by a diverse group of speakers.</p></td>
<td><p>4 emotions: general, joy, anger, and sadness.</p></td>
<td><p>Audio, Text</p></td>
<td><p>–</p></td>
<td><p>Korean</p></td>
<td><p>–</p></td>
<td><p>Partially open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/siddiquelatif/urdu-dataset">URDU-Dataset</a></p></td>
<td><p>2020</p></td>
<td><p>400 utterances by 38 speakers (27 male and 11 female).</p></td>
<td><p>4 emotions: angry, happy, neutral, and sad.</p></td>
<td><p>Audio</p></td>
<td><p>0.072 GB</p></td>
<td><p>Urdu</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1812.10411.pdf">Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages</a></p></td>
<td><p>Open</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.kaggle.com/a13x10/basic-arabic-vocal-emotions-dataset">BAVED</a></p></td>
<td><p>2020</p></td>
<td><p>1935 recording by 61 speakers (45 male and 16 female).</p></td>
<td><p>3 levels of emotion.</p></td>
<td><p>Audio</p></td>
<td><p>0.195 GB</p></td>
<td><p>Arabic</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/record/4066235">VIVAE</a></p></td>
<td><p>2020</p></td>
<td><p>non-speech, 1085 audio file by 11 speakers.</p></td>
<td><p>non-speech 6 emotions: achievement, anger, fear, pain, pleasure, and surprise with 3 emotional intensities (low, moderate, strong, peak).</p></td>
<td><p>Audio</p></td>
<td><p>0.0935 GB</p></td>
<td><p>Nonverbal (English)</p></td>
<td><p><a class="reference external" href="http://dx.doi.org/10.1037/emo0001048">The Variably Intense Vocalizations of Affect and Emotion (VIVAE) corpus prompts new perspective on nonspeech perception</a></p></td>
<td><p>Restricted</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://engineering.jhu.edu/nsa/vesus/">VESUS</a></p></td>
<td><p>2019</p></td>
<td><p>252 distinct phrases, each read by 10 actors totalling 6 hours of speech.</p></td>
<td><p>5 emotions: anger, happiness, sadness, fear and neutral.</p></td>
<td><p>Audio</p></td>
<td><p>–</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://engineering.jhu.edu/nsa/wp-content/uploads/2019/10/IS191413.pdf">VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English</a></p></td>
<td><p>Restricted</p></td>
<td><p>Academic EULA</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/abs/2403.02167">Morgan Emotional Speech Set</a></p></td>
<td><p>2019</p></td>
<td><p>999 spontaneous voice messages from 100 Spanish speakers, collected from real conversations on a messaging app.</p></td>
<td><p>Valence &amp; arrousal dimensions and 4 emotions: happiness, anger, sadness, and calmness.</p></td>
<td><p>Audio</p></td>
<td><p>0.192 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7203525/">Categorical and Dimensional Ratings of Emotional Speech: Behavioral Findings From the Morgan Emotional Speech Set</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/HuiZhangDB/PMEmo">PMEmo</a></p></td>
<td><p>2019</p></td>
<td><p>Dataset containing emotion annotations of 794 songs as well as the simultaneous electrodermal activity (EDA) signals. A Music Emotion Experiment was well-designed for collecting the affective-annotated music corpus of high quality, which recruited 457 subjects.</p></td>
<td><p>Valence, Arousal</p></td>
<td><p>Audio, EDA</p></td>
<td><p>1.3 GB</p></td>
<td><p>Chinese, English</p></td>
<td><p><a class="reference external" href="https://dl.acm.org/doi/10.1145/3206025.3206037">The PMEmo Dataset for Music Emotion Recognition</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://huisblog.cn/PMEmo//">CC BY-SA 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://db.sewaproject.eu/">SEWA</a></p></td>
<td><p>2019</p></td>
<td><p>more than 2000 minutes of audio-visual data of 398 people (201 male and 197 female) coming from 6 cultures.</p></td>
<td><p>emotions are characterized using valence and arousal.</p></td>
<td><p>Audio, Video</p></td>
<td><p>–</p></td>
<td><p>Chinese, English, German, Greek, Hungarian and Serbian</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1901.02839.pdf">SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild</a></p></td>
<td><p>Restricted</p></td>
<td><p><a class="reference external" href="https://db.sewaproject.eu/media/doc/eula.pdf">SEWA EULA</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://affective-meld.github.io/">MELD</a></p></td>
<td><p>2019</p></td>
<td><p>1400 dialogues and 14000 utterances from Friends TV series  by multiple speakers.</p></td>
<td><p>7 emotions: Anger, disgust, sadness, joy, neutral, surprise and fear.  MELD also has sentiment (positive, negative and neutral) annotation  for each utterance.</p></td>
<td><p>Audio, Video, Text</p></td>
<td><p>10.1 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1810.02508.pdf">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://github.com/declare-lab/MELD/blob/master/LICENSE">MELD: GPL-3.0 License</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/mansourehk/ShEMO">ShEMO</a></p></td>
<td><p>2019</p></td>
<td><p>3000 semi-natural utterances, equivalent to 3 hours and 25 minutes of speech data from online radio plays by 87 native-Persian speakers.</p></td>
<td><p>6 emotions: anger, fear, happiness, sadness, neutral and surprise.</p></td>
<td><p>Audio</p></td>
<td><p>0.101 GB</p></td>
<td><p>Persian</p></td>
<td><p><a class="reference external" href="https://link.springer.com/article/10.1007/s10579-018-9427-x">ShEMO: a large-scale validated database for Persian speech emotion detection</a></p></td>
<td><p>Open</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://zenodo.org/record/2544829">DEMoS</a></p></td>
<td><p>2019</p></td>
<td><p>9365 emotional and 332 neutral samples produced by 68 native speakers (23 females, 45 males).</p></td>
<td><p>7/6 emotions: anger, sadness, happiness, fear, surprise, disgust, and the secondary emotion guilt.</p></td>
<td><p>Audio</p></td>
<td><p>2.5 GB</p></td>
<td><p>Italian</p></td>
<td><p><a class="reference external" href="https://link.springer.com/epdf/10.1007/s10579-019-09450-y?author_access_token=5pf0w_D4k9z28TM6n4PbVPe4RwlQNchNByi7wbcMAY5hiA-aXzXNbZYfsMDDq2CdHD-w5ArAxIwlsk2nC_26pSyEAcu1xlKJ1c9m3JZj2ZlFmlVoCZUTcG3Hq2_2ozMLo3Hq3Y0CHzLdTxihQwch5Q%3D%3D">DEMoS: An Italian emotional speech corpus. Elicitation methods, machine learning, and perception</a></p></td>
<td><p>Restricted</p></td>
<td><p>EULA: End User License Agreement</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://m3c.web.auth.gr/research/aesdd-speech-emotion-recognition/">AESDD</a></p></td>
<td><p>2018</p></td>
<td><p>around 500 utterances by a diverse group of actors (over 5 actors) siumlating various emotions.</p></td>
<td><p>5 emotions: anger, disgust, fear, happiness, and sadness.</p></td>
<td><p>Audio</p></td>
<td><p>0.392 GB</p></td>
<td><p>Greek</p></td>
<td><p><a class="reference external" href="https://www.researchgate.net/publication/326005164_Speech_Emotion_Recognition_for_Performance_Interaction">Speech Emotion Recognition for Performance Interaction</a></p></td>
<td><p>Open</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://mega.nz/#F!KBp32apT!gLIgyWf9iQ-yqnWFUFuUHg!mYwUnI4K">Emov-DB</a></p></td>
<td><p>2018</p></td>
<td><p>Recordings for 4 speakers- 2 males and 2 females.</p></td>
<td><p>The emotional styles are neutral, sleepiness, anger, disgust and amused.</p></td>
<td><p>Audio</p></td>
<td><p>5.88 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1806.09514.pdf">The emotional voices database: Towards controlling the emotion dimension in voice generation systems</a></p></td>
<td><p>Open</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www2.informatik.uni-hamburg.de/wtm/OMG-EmotionChallenge/">OMG Emotion</a></p></td>
<td><p>2018</p></td>
<td><p>420 relatively long emotion videos with an average length of 1 minute, collected from a variety of Youtube channels.</p></td>
<td><p>7 emotions:anger, disgust, fear, happy, sad, surprise and neutral. Plus valence, arousal.</p></td>
<td><p>Audio, Video</p></td>
<td><p>–</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1803.05434">The OMG-Emotion Behavior Dataset</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://zenodo.org/record/1188976#.XrC7a5NKjOR">RAVDESS</a></p></td>
<td><p>2018</p></td>
<td><p>7356 recordings by 24 actors.</p></td>
<td><p>7 emotions: calm, happy, sad, angry, fearful, surprise, and disgust</p></td>
<td><p>Audio, Video</p></td>
<td><p>24.8 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196391">The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.kaggle.com/tli725/jl-corpus">JL corpus</a></p></td>
<td><p>2018</p></td>
<td><p>2400 recording of 240 sentences by 4 actors (2 males and 2 females).</p></td>
<td><p>5 primary emotions: angry, sad, neutral, happy, excited. 5 secondary emotions: anxious, apologetic, pensive, worried, enthusiastic.</p></td>
<td><p>Audio</p></td>
<td><p>1.9 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1349.pdf">An Open Source Emotional Speech Corpus for Human Robot Interaction Applications</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/publicdomain/zero/1.0/">CC0 1.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://zenodo.org/record/1478765">CaFE</a></p></td>
<td><p>2018</p></td>
<td><p>6 different sentences by 12 speakers (6 fmelaes + 6 males).</p></td>
<td><p>7 emotions: happy, sad, angry, fearful, surprise, disgust and neutral. Each emotion is acted in 2 different intensities.</p></td>
<td><p>Audio</p></td>
<td><p>2 GB</p></td>
<td><p>French (Canadian)</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/record/1326428">EmoFilm</a></p></td>
<td><p>2018</p></td>
<td><p>1115 audio instances sentences extracted from various films.</p></td>
<td><p>5 emotions: anger, contempt, happiness, fear, and sadness.</p></td>
<td><p>Audio</p></td>
<td><p>0.277 GB</p></td>
<td><p>English, Italian, Spanish</p></td>
<td><p><a class="reference external" href="https://pdfs.semanticscholar.org/e70e/fcf7f5b4c366a7b7e2c16267d7f7691a5391.pdf">Categorical vs Dimensional Perception of Italian Emotional Speech</a></p></td>
<td><p>Restricted</p></td>
<td><p>EULA: End User License Agreement</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.kaggle.com/suso172/arabic-natural-audio-dataset">ANAD</a></p></td>
<td><p>2018</p></td>
<td><p>1384 recording by multiple speakers.</p></td>
<td><p>3 emotions: angry, happy, surprised.</p></td>
<td><p>Audio</p></td>
<td><p>2 GB</p></td>
<td><p>Arabic</p></td>
<td><p><a class="reference external" href="https://data.mendeley.com/datasets/xm232yxf7t/1">Arabic Natural Audio Dataset</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://zenodo.org/record/3727593">EmoSynth</a></p></td>
<td><p>2018</p></td>
<td><p>144 audio file labelled by 40 listeners.</p></td>
<td><p>Emotion (no speech) defined in regard of valence and arousal.</p></td>
<td><p>Audio</p></td>
<td><p>0.1034 GB</p></td>
<td><p>–</p></td>
<td><p><a class="reference external" href="https://dl.acm.org/doi/10.1145/3243274.3243277">The Perceived Emotion of Isolated Synthetic Audio: The EmoSynth Dataset and Results</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.amir-zadeh.com/datasets">CMU-MOSEI</a></p></td>
<td><p>2018</p></td>
<td><p>65 hours of annotated video from more than 1000 speakers and 250 topics.</p></td>
<td><p>6 Emotion (happiness, sadness, anger,fear, disgust, surprise) + Likert scale.</p></td>
<td><p>Audio, Video</p></td>
<td><p>190.1 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1802.00923.pdf">Multi-attention Recurrent Network for Human Communication Comprehension</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://github.com/A2Zadeh/CMU-MultimodalSDK/blob/master/LICENSE.txt">CMU-MOSEI License</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://sites.google.com/view/verbodatabase/home">VERBO</a></p></td>
<td><p>2018</p></td>
<td><p>14 different phrases by 12 speakers (6 female + 6 male) for a total of 1167 recordings.</p></td>
<td><p>7 emotions: Happiness, Disgust, Fear, Neutral, Anger, Surprise, Sadness</p></td>
<td><p>Audio</p></td>
<td><p>–</p></td>
<td><p>Portuguese</p></td>
<td><p><a class="reference external" href="https://thescipub.com/pdf/jcssp.2018.1420.1430.pdf">VERBO: Voice Emotion Recognition dataBase in Portuguese Language</a></p></td>
<td><p>Restricted</p></td>
<td><p>Available for research purposes only</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.amir-zadeh.com/datasets">CMU-MOSI</a></p></td>
<td><p>2017</p></td>
<td><p>2199 opinion utterances with annotated sentiment.</p></td>
<td><p>Sentiment annotated between very negative to very positive in seven Likert steps.</p></td>
<td><p>Audio, Video</p></td>
<td><p>4.3 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1802.00923.pdf">Multi-attention Recurrent Network for Human Communication Comprehension</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://github.com/A2Zadeh/CMU-MultimodalSDK/blob/master/LICENSE.txt">CMU-MOSI License</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Improv.html">MSP-IMPROV</a></p></td>
<td><p>2017</p></td>
<td><p>20 sentences by 12 actors.</p></td>
<td><p>4 emotions: angry, sad, happy, neutral, other, without agreement</p></td>
<td><p>Audio, Video</p></td>
<td><p>3.4 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://ecs.utdallas.edu/research/researchlabs/msp-lab/publications/Busso_2017.pdf">MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception</a></p></td>
<td><p>Restricted</p></td>
<td><p>Academic License &amp; Commercial License</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/CheyneyComputerScience/CREMA-D">CREMA-D</a></p></td>
<td><p>2017</p></td>
<td><p>7442 clip of 12 sentences spoken by 91 actors (48 males and 43 females).</p></td>
<td><p>6 emotions: angry, disgusted, fearful, happy, neutral, and sad</p></td>
<td><p>Audio, Video</p></td>
<td><p>0.607 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/">CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://github.com/CheyneyComputerScience/CREMA-D/blob/master/LICENSE.txt">Open Database License &amp; Database Content License</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://espace.library.uq.edu.au/view/UQ:446541">Example emotion videos used in investigation of emotion perception in schizophrenia</a></p></td>
<td><p>2017</p></td>
<td><p>6 videos:Two example videos from each emotion category (angry, happy and neutral) by one female speaker.</p></td>
<td><p>3 emotions: angry, happy and neutral.</p></td>
<td><p>Audio, Video</p></td>
<td><p>0.063 GB</p></td>
<td><p>English</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://guides.library.uq.edu.au/deposit_your_data/terms_and_conditions">Permitted Non-commercial Re-use with Acknowledgment</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://voice.fub.it/activities/corpora/emovo/index.html">EMOVO</a></p></td>
<td><p>2014</p></td>
<td><p>6 actors  who  played  14  sentences.</p></td>
<td><p>6 emotions: disgust, fear, anger, joy, surprise, sadness.</p></td>
<td><p>Audio</p></td>
<td><p>0.355 GB</p></td>
<td><p>Italian</p></td>
<td><p><a class="reference external" href="https://core.ac.uk/download/pdf/53857389.pdf">EMOVO Corpus: an Italian Emotional Speech Database</a></p></td>
<td><p>Open</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://diuf.unifr.ch/main/diva/recola/download.html">RECOLA</a></p></td>
<td><p>2013</p></td>
<td><p>3.8 hours of recordings by 46 participants.</p></td>
<td><p>negative and positive sentiment (valence and arousal).</p></td>
<td><p>Audio, Video</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p><a class="reference external" href="https://drive.google.com/file/d/0B2V_I9XKBODhNENKUnZWNFdVXzQ/view">Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions</a></p></td>
<td><p>Restricted</p></td>
<td><p>Academic License &amp; Commercial License</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.unige.ch/cisa/gemep">GEMEP corpus</a></p></td>
<td><p>2012</p></td>
<td><p>Videos10 actors portraying 10 states.</p></td>
<td><p>12 emotions: amusement, anxiety, cold anger (irritation), despair, hot anger (rage),  fear (panic), interest, joy (elation), pleasure(sensory), pride, relief, and sadness. Plus, 5 additional emotions: admiration, contempt, disgust, surprise, and tenderness.</p></td>
<td><p>Audio, Video</p></td>
<td><p>–</p></td>
<td><p>French</p></td>
<td><p><a class="reference external" href="https://www.researchgate.net/publication/51796867_Introducing_the_Geneva_Multimodal_Expression_Corpus_for_Experimental_Research_on_Emotion_Perception">Introducing the Geneva Multimodal Expression Corpus for Experimental Research on Emotion Perception</a></p></td>
<td><p>Restricted</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://sites.google.com/site/ogcorpus/home/en">OGVC</a></p></td>
<td><p>2012</p></td>
<td><p>9114 spontaneous utterances and 2656 acted utterances by 4 professional actors (two male and two female).</p></td>
<td><p>9 emotional states: fear, surprise, sadness, disgust, anger, anticipation, joy, acceptance and the neutral state.</p></td>
<td><p>Audio</p></td>
<td><p>5.3 GB</p></td>
<td><p>Japanese</p></td>
<td><p><a class="reference external" href="https://www.jstage.jst.go.jp/article/ast/33/6/33_E1175/_pdf">Naturalistic emotional speech collectionparadigm with online game and its psychological and acoustical assessment</a></p></td>
<td><p>Restricted</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.ultes.eu/ressources/lego-spoken-dialogue-corpus/">LEGO corpus</a></p></td>
<td><p>2012</p></td>
<td><p>347 dialogs with 9,083 system-user exchanges.</p></td>
<td><p>Emotions classified as garbage, non-angry, slightly angry and very angry.</p></td>
<td><p>Audio</p></td>
<td><p>1.1 GB</p></td>
<td><p>–</p></td>
<td><p><a class="reference external" href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/333_Paper.pdf">A Parameterized and Annotated Spoken Dialog Corpus of the CMU Let’s Go Bus Information System</a></p></td>
<td><p>Open</p></td>
<td><p>License available with the data. Free of charges for research purposes only.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://semaine-db.eu/">SEMAINE</a></p></td>
<td><p>2012</p></td>
<td><p>95 dyadic conversations from 21 subjects. Each subject converses with another playing one of four characters with emotions.</p></td>
<td><p>5 FeelTrace annotations: activation, valence, dominance, power, intensity</p></td>
<td><p>Audio, Video, Text</p></td>
<td><p>104 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/5959155">The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent</a></p></td>
<td><p>Restricted</p></td>
<td><p>Academic EULA</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://kahlan.eps.surrey.ac.uk/savee/Database.html">SAVEE</a></p></td>
<td><p>2011</p></td>
<td><p>480 British English utterances by 4 males actors.</p></td>
<td><p>7 emotions: anger, disgust, fear, happiness, sadness, surprise and neutral.</p></td>
<td><p>Audio, Video</p></td>
<td><p>–</p></td>
<td><p>English (British)</p></td>
<td><p><a class="reference external" href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/pub/ma10/HaqJackson_MachineAudition10_approved.pdf">Multimodal Emotion Recognition</a></p></td>
<td><p>Restricted</p></td>
<td><p>Free of charges for research purposes only.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://tspace.library.utoronto.ca/handle/1807/24487">TESS</a></p></td>
<td><p>2010</p></td>
<td><p>2800 recording by 2 actresses.</p></td>
<td><p>7 emotions: anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral.</p></td>
<td><p>Audio</p></td>
<td><p>–</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://www.semanticscholar.org/paper/BEHAVIOURAL-FINDINGS-FROM-THE-TORONTO-EMOTIONAL-SET-Dupuis-Pichora-Fuller/d7f746b3aee801a353b6929a65d9a34a68e71c6f/figure/2">BEHAVIOURAL FINDINGS FROM THE TORONTO EMOTIONAL SPEECH SET</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://metashare.ut.ee/repository/download/4d42d7a8463411e2a6e4005056b40024a19021a316b54b7fb707757d43d1a889/">EEKK</a></p></td>
<td><p>2007</p></td>
<td><p>26 text passage read by 10 speakers.</p></td>
<td><p>4 main emotions: joy, sadness, anger and neutral.</p></td>
<td><p>–</p></td>
<td><p>0.352 GB</p></td>
<td><p>Estonian</p></td>
<td><p><a class="reference external" href="https://www.researchgate.net/publication/261724574_Estonian_Emotional_Speech_Corpus_Release_1">Estonian Emotional Speech Corpus</a></p></td>
<td><p>Open</p></td>
<td><p><a class="reference external" href="https://metashare.ut.ee/repository/download/4d42d7a8463411e2a6e4005056b40024a19021a316b54b7fb707757d43d1a889/">CC-BY license</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://sail.usc.edu/iemocap/iemocap_release.htm">IEMOCAP</a></p></td>
<td><p>2007</p></td>
<td><p>12 hours of audiovisual data by 10 actors in 5 sessions.</p></td>
<td><p>Full: neutral state; happiness; sadness; anger; surprise; fear; disgust; frustration; excited; other. Balance 5 emotions: happiness, anger, sadness, frustration and neutral. Three dimensions: valence, arousal, dominance</p></td>
<td><p>Audio, Video, Text</p></td>
<td><p>17.7 GB</p></td>
<td><p>English</p></td>
<td><p><a class="reference external" href="https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf">IEMOCAP: Interactive emotional dyadic motion capture database</a></p></td>
<td><p>Restricted</p></td>
<td><p><a class="reference external" href="https://sail.usc.edu/iemocap/Data_Release_Form_IEMOCAP.pdf">IEMOCAP license</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://research.nii.ac.jp/src/en/Keio-ESD.html">Keio-ESD</a></p></td>
<td><p>2006</p></td>
<td><p>A set of human speech with vocal emotion spoken by a Japanese male speaker.</p></td>
<td><p>47 emotions including angry, joyful, disgusting, downgrading, funny,  worried, gentle, relief, indignation, shameful, etc.</p></td>
<td><p>Audio</p></td>
<td><p>0.0435 GB</p></td>
<td><p>Japanese</p></td>
<td><p><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.420.8899&amp;rep=rep1&amp;type=pdf">EMOTIONAL SPEECH SYNTHESIS USING SUBSPACE CONSTRAINTS IN PROSODY</a></p></td>
<td><p>Restricted</p></td>
<td><p>Available for research purposes only.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://emodb.bilderbar.info/index-1280.html">EMO-DB</a></p></td>
<td><p>2005</p></td>
<td><p>800 recording spoken by 10 actors (5 males and 5 females).</p></td>
<td><p>7 emotions: anger, neutral, fear, boredom, happiness, sadness, disgust.</p></td>
<td><p>Audio</p></td>
<td><p>0.049 GB</p></td>
<td><p>German</p></td>
<td><p><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.8506&amp;rep=rep1&amp;type=pdf">A Database of German Emotional Speech</a></p></td>
<td><p>Open</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="http://www.enterface.net/enterface05/docs/results/databases/project2_database.zip">eNTERFACE05</a></p></td>
<td><p>2005</p></td>
<td><p>Videos by 42 subjects, coming from 14 different nationalities.</p></td>
<td><p>6 emotions: anger, fear, surprise, happiness, sadness and disgust.</p></td>
<td><p>Audio, Video</p></td>
<td><p>0.8 GB</p></td>
<td><p>German</p></td>
<td><p>–</p></td>
<td><p>Open</p></td>
<td><p>Free of charges for research purposes only.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="http://kom.aau.dk/~tb/speech/Emotions/">DES</a></p></td>
<td><p>2002</p></td>
<td><p>4 speakers (2 males and 2 females).</p></td>
<td><p>5 emotions: neutral,  surprise,  happiness,  sadness  and  anger</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>Danish</p></td>
<td><p><a class="reference external" href="http://kom.aau.dk/~tb/speech/Emotions/des.pdf">Documentation of the Danish Emotional Speech Database</a></p></td>
<td><p>–</p></td>
<td><p>–</p></td>
</tr>
</tbody>
</table>
</section>
<section id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>Swain, Monorama &amp; Routray, Aurobinda &amp; Kabisatpathy, Prithviraj, Databases, features and classifiers for speech emotion recognition: a review, International Journal of Speech Technology, <a class="reference external" href="https://www.researchgate.net/publication/322602563_Databases_features_and_classifiers_for_speech_emotion_recognition_a_review#pf19">paper1</a></p></li>
<li><p>Dimitrios Ververidis and Constantine Kotropoulos, A State of the Art Review on Emotional Speech Databases, Artificial Intelligence &amp; Information Analysis Laboratory, Department of Informatics Aristotle, University of Thessaloniki, <a class="reference external" href="http://poseidon.csd.auth.gr/papers/PUBLISHED/CONFERENCE/pdf/Ververidis2003b.pdf">paper2</a></p></li>
<li><p>Florian Eyben, Anton Batliner and Bjoern Schulle, Towards a standard set of acoustic features for the processing of emotion in speech, Acoustical society of America, <a class="reference external" href="https://asa.scitation.org/doi/pdf/10.1121/1.4739483">paper3</a></p></li>
<li><p>Aeluri Pramod Reddy and V Vijayarajan, Extraction of Emotions from Speech-A Survey, VIT University, International Journal of Applied Engineering Research, <a class="reference external" href="https://www.ripublication.com/ijaer17/ijaerv12n16_46.pdf">paper4</a></p></li>
<li><p>Emotional Speech Databases, <a class="reference external" href="https://link.springer.com/content/pdf/bbm%3A978-90-481-3129-7%2F1.pdf">document</a></p></li>
<li><p>Expressive Synthetic Speech, <a class="reference external" href="http://emosamples.syntheticspeech.de/">http://emosamples.syntheticspeech.de/</a></p></li>
</ul>
</section>
<section id="contributing">
<h1>Contributing<a class="headerlink" href="#contributing" title="Permalink to this headline">#</a></h1>
<p>All contributions are welcome!
If you know a dataset that belongs here (see <a class="reference external" href="https://github.com/SuperKogito/SER-datasets/blob/master/CONTRIBUTING.md#criteria">criteria</a>) but is not listed, please feel free to add it.
For more information on Contributing, please refer to <a class="reference external" href="https://github.com/SuperKogito/SER-datasets/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p>If you notice a typo or a mistake, please <a class="reference external" href="https://github.com/SuperKogito/SER-datasets/issues/new">report this as an issue</a> and help us improve the quality of this list.</p>
</section>
<section id="disclaimer">
<h1>Disclaimer<a class="headerlink" href="#disclaimer" title="Permalink to this headline">#</a></h1>
<p>The maintainer and the contributors try their best to keep this list up-to-date, and to only include working links (using automated verification with the help of the <a class="reference external" href="https://github.com/marketplace/actions/urlchecker-action">urlchecker-action</a>).
However, we cannot guarantee that all listed links are up-to-date. Read more in <a class="reference external" href="https://github.com/SuperKogito/SER-datasets/blob/master/DISCLAIMER.md">DISCLAIMER.md</a>.</p>
</section>
<section id="recommended-tools">
<h1>Recommended tools<a class="headerlink" href="#recommended-tools" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/felixbur/nkululeko">Nkululeko</a></p></li>
</ul>
<p>This toolkit has a <a class="reference external" href="https://github.com/felixbur/nkululeko/tree/main/data">data</a> directory with each python-preprocessing script that can load most datasets in this list.
The processing script there will split the data into train, validation, and test sets, and save them as CSV files with file paths and labels.
Then, you can make make experiments to detect emotions from speech using that dataset with Nkululeko or other tools.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/Strong-AI-Lab/emotion">ERTK</a></p></li>
</ul>
<p>Similar to Nkululeko, ERTK (emotion recognition toolkit) also has <a class="reference external" href="https://github.com/Strong-AI-Lab/emotion/tree/master/datasets">dataset</a> directory that can load most datasets in this list.</p>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
<footer class="footer mt-5 mt-md-0">
      <!-- Site footer -->
      <div class="container">
        <div class="row">
          <div class="col-xs-6 col-md-6">
            <ul class="footer-links">
              <li>
                    &copy; Copyright 2020-2024, Ayoub Malek.<br/>
                  This blog is created using <a href="http://sphinx-doc.org/">Sphinx</a> and the tools from <a href="https://executablebooks.org/en/latest/">The Executable Book Project</a>.<br/>
              </li>
            </ul>
          </div>

          <div class="col-xs-6 col-md-2">
            <ul class="footer-links">
              <li><a href="https://superkogito.github.io/policy/privacy_policy.html">Privacy Policy</a></li>
              <li><a href="https://superkogito.github.io/policy/cookies_policy.html">Cookies policy</a></li>
            </ul>
          </div>

          <div class="col-xs-6 col-md-2">
            <ul class="footer-links">
              <li><a href="https://superkogito.github.io/policy/terms_and_conditions.html">Terms and conditions</a></li>
              <li><a href="https://superkogito.github.io/policy/disclaimer.html">Disclaimer</a></li>
            </ul>
          </div>

          <div class="col-xs-6 col-md-2">
            <ul class="footer-links">
              <li><a href="https://superkogito.github.io/policy/contribute.html">Contribute</a></li>
              <li><a href="https://superkogito.github.io/sitemap.xml">Sitemap</a></li>
            </ul>
          </div>

    </div>
  </footer>
  </body>
</html>